# Interview Notes – P05

## Participant Info
| Field | Notes |
|-------|--------|
| Stakeholder type | Parent (Mum) of children aged 6 and 9 |
| Ethnic Background | Chinese Singaporean |
| Age group | 30's |
| Interview date | 21 Dec 2025 |
| Interviewer | Natasha |
| Consent status | Recording allowed: Yes |

---

## Key Discussion Points
### Current Behaviors
- Kids have their own iPads; no personal phones.
- Screen use is primarily for structured learning/lessons (some from China; also local English/Math classes).
- Limited play allowance: son plays games about once a week; parent monitors what he plays.
- Uses tracking for child safety (AirTag in bag) due to child commuting and occasional unplanned route changes.
- Prefers Apple ecosystem (iPhone/Apple Watch) for stability and stronger parental controls/location tracking.
- "TV Time" for both kids is usually limited to 1 episode on Netflix, around 25mins / day; occasionally 1 hour.
- Curates content sources: prefers Netflix Kids; avoids YouTube/YouTube Kids except when needed (e.g., music/violin content with YouTube Premium to avoid ads).
- Allows both kids to trade "TV Time" for game time on the Nintendo Switch 2 with age-appropriate games like Mario Kart, Pokémon Legends: Z-A, Sonic X Shadow Generations.
- In line with ad-avoidance, I also highly prefer paid games as free games, especially those on iPad, are often full of ads, micro-transactions and game currency (e.g. Roblox's robux). Hence the choice of Switch 2 as a gaming platform vs iPad. 
- Prefers to not expose my developing children's minds to psychological manipulation.

### Pain Points / Frustrations
- Exposure to harmful/stimulating content can occur indirectly via peers (friends with phones; older kids at playground).
- Hard to control what kids see outside the home; parent feels efforts at protection are undermined by social transmission of memes/videos.
- Regret about early YouTube exposure for first child; feels it impacted attention span and is difficult to reverse.
- Distrust toward large tech platforms’ “for kids” claims; feels betrayed by engagement-driven design.
- Time-limit enforcement is emotionally/relationally hard (booting a child mid-game feels harsh; “5 minutes becomes an hour”).
- Ongoing tension between monitoring for safety vs privacy/trust as kids grow.

### Desired Features / Needs
- Controlled introduction to communication tech (calls/text) for safety and coordination (e.g., child commuting).
- Ability to:
  - Disable app installation / whitelist only approved apps.
  - Monitor/approve contacts and chats (e.g., WhatsApp) for primary-school age.
  - Set app time limits with clear enforcement options.
  - Track location reliably with low setup friction.
- Better child-safe content standards and reduced over-stimulation.
- Tools/UX to help kids learn skills to handle harmful content when it appears (not just block everything).

### Safety & Trust Requirements
- Strong concern about social platforms and user-generated video ecosystems (memes spreading “like viruses”).
- Roblox seen as risky due to predators; wants to allow fun but with supervision and guardrails.
- Ads are viewed as harmful (“ads are sewage”); prefers paid options to remove ad exposure.
- Wants products that are well-tested before exposing children; wary of AI chat products for young kids.
- Emphasizes that safety is not only technical: child must trust parent enough to disclose incidents.

### Opportunities / Ideas
- “Graduated exposure” model: introduce devices/features in stages (calls-only -> calls+text -> limited messaging -> broader access) with coaching and review.
- Incident-based learning workflow: when exposure happens, provide steps kids can take (e.g., dislike/report/block; talk to a trusted adult).
- “Trust contract” patterns: shared rules + periodic check-ins; transparency about monitoring expectations for young kids.
- Content quality rating guidance for parents (e.g., why Netflix Kids feels higher quality vs fast-cut, high-stimulation YouTube content).
- Support for real-world safety and logistics (commute coordination, location sharing) without opening up the wider app ecosystem.

---

## Direct Quotes
> “memes and videos are like viruses, they live on in people’s heads”

> “as a parent it is pretty frustrating - you are doing your best to get your kids protected, but when you go out it is difficult to protect”

> “youtube kids for son… after awhile i realized the word 'kids' behind youtube doesn't really mean much”

> “these tech companies they are not trustable”

> “would rather control the fire so i can control the damage”

> “for primary sch kids, dont think they have the right to privacy at the moment because they cant handle dangers by themselves”

> “ads are sewage”

---

## Emotional Tone (optional)
| Feeling observed | Evidence |
|------------------|-----------|
| Concerned | Repeated emphasis on uncontrollable peer exposure, predators on platforms, and fear of harmful content (incl. sexting/violent content). |
| Frustrated / Betrayed | Strong language about YouTube Kids/tech companies optimizing for money/engagement; regret about early exposure. |
| Pragmatic / Determined | Wants controlled exposure rather than isolation; believes in building skills and a trust relationship. |

---

## Potential Themes Tags
`#safety` `#trust` `#parental_controls` `#screen_time` `#content_quality` `#ads` `#roblox` `#peer_exposure` `#graduated_exposure` `#ai_wary`

---

## Follow-Ups
- How does she currently configure iOS Screen Time (limits, downtime, app restrictions), and what gaps does she still feel?
- What specific Roblox safety controls (chat settings, friends list, age settings) does she use or want?
- What would an ideal “calls/text-only” device experience look like for a primary-school child (including emergency, location, and contact approval)?
- What are her criteria for “well-tested” AI products (certifications, school endorsement, transparency, evidence)?
