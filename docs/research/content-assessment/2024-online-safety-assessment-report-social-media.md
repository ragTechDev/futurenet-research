# 2024 Online Safety Assessment Report 2024: Designated Social Media Services

See here for the [report](https://www.imda.gov.sg/-/media/imda/files/regulations-and-licensing/regulations/online-safety/online-safety-assessment-report-2024-designated-social-media-services.pdf).

## Executive Summary
The inaugural Online Safety Assessment Report aims to inform Singapore users of the online safety
measures Designated Social Media Services (DSMSs) have in place, as required by the Code of
Practice for Online Safety – Social Media Services (Code). It assesses the comprehensiveness and
effectiveness of these measures to mitigate risks from harmful content, and highlights areas for
improvement. This allows users to make informed decisions about the risks and available safety
measures, and ensures that DSMSs are accountable for providing a safe user experience. The six
DSMSs are Facebook, HardwareZone, Instagram, TikTok, X and YouTube.

## Code of Practice for Online Safety – Social Media Services
The Code was published in July 2023. The six categories of harmful content DSMSs must
address are:
1. Sexual content
2. Violent content
3. Suicide and self-harm content
4. Cyberbullying content
5. Content endangering public health
6. Content facilitating vice and organised crime

The Code outlines system-level measures to minimise users’ exposure to harmful content. The Code
takes an outcomes-based approach and gives DSMSs the flexibility to design their measures to meet
the intent. See Annex A for the full text of the Code. The Code requires DSMSs to:

### a. Enhance online safety by minimising users’ exposure to harmful content, with enhanced
protections for children (users under the age of 18)
 - i. Put in place systems and processes to address harmful content, including community guidelines
and effective content moderation measures.
 - ii. Empower users with tools to manage their own safety.
 - iii.Proactively detect and swiftly remove child sexual exploitation and abuse material (CSEM) and
terrorism content.
 - iv.Have enhanced protections for children including age-appropriate policies and tools for
parents/guardians to manage their children’s safety.

### b. Empower users with effective and easy-to-use mechanisms to report harmful content
 - i. Take appropriate action on user reports in a timely and diligent manner and inform these
users of the decision and any action taken in response to the reports.

### c. Ensure transparency and accountability to users by submitting annual online safety reports
 - i. The reports must contain clear information on the DSMSs’ safety measures, supported by suitable
data that reflects the impact of their safety efforts in Singapore. This will enable users to make
informed choices on which DSMSs would be best placed to provide safe user experiences.

## Methodology
DSMSs were assessed on whether their measures were comprehensive and effective in achieving
the Code’s safety outcomes. To assess effectiveness, test accounts were set up to simulate realworld user experiences. “Mystery shopper” tests were also conducted, by reporting harmful content
that violated the DSMSs’ own community guidelines, to assess if user reporting and resolution
mechanisms were effective. Please refer to the main report for details on the methodology. 

## Online Safety Ratings
Each DSMS received (a) an Overall Rating and (b) Ratings for Individual Sections of the Code. Overall,
the ratings show that DSMSs performed better in User safety measures for all users, and Accountability.
They were weaker in User safety measures for children, and User reporting and resolution.

| DSMS Overall Rating | Section Ai: User safety measures for all end-users | Section Aii: User safety measures for children | Section B: User reporting & resolution | Section C: Accountability
| --- | --- | --- | --- | --- |
| Facebook | 3.5 | 5 | 4 | 2.5 | 3.5 |
| HardwareZone | 3.5 | 5 | 2.5 | 4 | 5 |
| Instagram | 3.5 | 5 | 5 | 2 | 3.5 |
| TikTok | 4 | 5 | 5 | 2.5 | 5 |
| X | 2.5 | 2.5 | 2 | 2 | 5 |
| YouTube | 3.5 | 5 | 4 | 3 | 3.5 |

__Note: Scores are based on a scale of 1 to 5, with 5 being the highest rating.__

## Summary of Key Findings by Section

### Section Ai: User safety measures for all end-users
DSMSs have largely put in place the required user safety measures, including:

- a. Community guidelines covering the six categories of harmful content in the Code, and both human and automated content moderation measures.
- b. Tools for users to manage their own safety, such as tools to restrict visibility of harmful content and/or unwanted comments limit the visibility of user accounts, posted content and interactions with others, and limit location sharing where applicable.
- c. Easily accessible and understood online safety information such as Help Centres and pages. Facebook, Instagram, TikTok and YouTube also implemented and supported additional programmes and initiatives to educate and raise awareness of such information in Singapore.
- d. Singapore-based safety resources for users who searched high-risk terms related to suicide and self-harm. Some DSMSs offered additional resources for search terms related to domestic violence, sexual violence and cyberbullying. Facebook, Instagram
and TikTok also offered additional resources such as options to contact a friend for support, or links to professional and digital wellness resources. 

X needs to improve the effectiveness of its efforts to detect and remove CSEM on its service. CSEM
is a very egregious type of harm, and the Code requires DSMSs to use technology to proactively
detect and swiftly remove CSEM before users encounter such content. X’s own publicly stated policy
“prohibits any content that depicts or promotes child sexual exploitation”. In its annual report, X
stated that it proactively detected and removed 6 pieces of CSEM originating from Singapore.
However, our tests detected considerably more cases of CSEM originating from Singapore on X
during the same period. X will therefore need to provide IMDA with an update on steps taken to
improve the effectiveness of its measures against CSEM.

### Section Aii: User safety measures for children
DSMSs should do more to improve the effectiveness of
their measures to protect children from harmful and ageinappropriate content.

- a. All DSMSs had community guidelines appropriate for
children. However, the enforcement of these community
guidelines should be improved as children’s accounts
could still access harmful and age-inappropriate content
on some DSMSs.
 - i. Facebook and YouTube had instances where children’s accounts could access content that should have been restricted under their own community guidelines. These include digital imagery of adult content on Facebook, and videos with sexually suggestive imagery on YouTube.
 - ii. X did not effectively enforce its policies to restrict children’s accounts from viewing adult sexual content. X’s own publicly stated policy states that it restricts “viewers who are under 18, or who do not include a birth date on their profile, from viewing adult content”. However, our tests found that children’s accounts could easily find and access explicit adult sexual content, especially hardcore pornography, with simple search terms. X will need to provide IMDA with an update on steps taken to improve the effectiveness of its measures to prevent children from accessing age-inappropriate content, especially adult sexual content.

- b. HardwareZone’s Terms of Service prohibit users under the age of 18 from accessing the service. However, its age-gating measure to enforce this was easily bypassed. HardwareZone should
either effectively restrict children from accessing its service or put in place comprehensive safety measures for children as required by the Code. HardwareZone will need to provide IMDA with an
update on this.

Today, the Code already requires DSMSs to ensure that children receive age-appropriate content
and experiences. However, we have allowed DSMSs to decide how best to achieve this outcome, and
not required them to implement age assurance measures. From our monitoring, IMDA assesses that
age assurance technology has improved considerably. IMDA has already made it a requirement for
Designated App Distribution Services to implement age assurance measures to ensure children and
youth do not download apps that are inappropriate for their age. We are also studying how Social
Media Services should use age assurance technology to better protect children and youth online.

### Section B: User reporting & resolution
DSMSs have prioritised technologies to automatically
detect and remove harmful content at-scale. However,
it is important that they take user reports seriously as it
is a critical pathway for recourse for a user encountering
online harms.

DSMSs should improve the effectiveness and timeliness of their responses to user reports. The Code
requires DSMSs to take appropriate action on user reports of harmful content that violates their
community guidelines in a timely and diligent manner.

- a. Our “Mystery shopper” tests (see results in Table 1) found that most DSMSs took appropriate actions
on only approximately 50% or less of the content that violated their own community guidelines.
When we subsequently notified the DSMSs to re-review the remaining pieces of harmful content, all
were found to be violating their own community guidelines and subsequently removed. This means
that a significant proportion of legitimate user reports were not actioned on in the first instance.

- b. Most DSMSs also took an average of 5 days or more to act on user reports of harmful content that
violated their own community guidelines. This was considerably longer than what was stated in their
annual reports.

These findings corroborate the Ministry of Digital Development and Information’s 2024 Online
Safety Poll which found that between 78% to 86% of respondents faced issues with user reporting on
DSMSs, including: (a) the service did not take down the harmful online content reported or disable the
account responsible for it, (b) the service did not provide updates on the outcomes of their reports
and (c) the service allowed the removed content to be reposted. All DSMSs, with the exception of
HardwareZone, will need to provide an update on steps taken to improve their effectiveness and
timeliness of user reporting measures. 

### Section C: Accountability
DSMSs should be transparent and accountable to users
by providing clear information on how they are keeping
their service safe for users. This allows users to make
informed choices on which DSMS to use. As a baseline,
the Code requires DSMSs to provide information on their
measures and data to demonstrate the effectiveness of
these measures. This is to be done through annual online
safety reports. 

In their first annual online safety reports, all DSMSs met this baseline standard. DSMSs submitted
their annual reports on time and with clear information. In addition, the Code places emphasis on
data which reflects the impact of the DSMSs’ online safety efforts in Singapore. While some DSMSs
were able to provide such data, other DSMSs did not or were unable to do so. Facebook, Instagram
and YouTube should have provided data for Singapore users to understand the effectiveness and
timeliness of their user reporting and resolution mechanisms. It is important for Singapore users
to have access to up-to-date online safety data relevant to them. DSMSs should therefore improve
on information transparency to Singapore users.

## Conclusion
The DSMSs’ online safety measures were generally comprehensive. However, there are areas
which require improvement, particularly in the effectiveness of these measures. DSMSs will need
to provide updates on the steps taken to improve their areas of weakness in their next annual
online safety reports.